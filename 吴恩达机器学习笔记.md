# 吴恩达机器学习笔记
### 引言
学习AndrewNg的机器学习有一周的时间了，收获了不少东西。因此想通过这个学习笔记来将所学的知识从头梳理一遍并做一个总结。
> ## 1. 绪论
> ### 1.1 什么是机器学习
> 这里主要有两种定义：

>Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed.<br>
这个定义有点不正式但提出的时间最早，来自于一个懂得计算机编程的下棋菜鸟，编程使得计算机通过不断的对弈，不断地计算布局的好坏来“学习”，从而积累经验，这样，这个计算机程序成为了一个厉害的棋手。

>Tom Mitchell (1998) Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some **task T** and some performance measure P, if its **performance on T**, as measured by P, improves with **experience E**.<br>
Tom Mitchell 的定义更现代，也有点拗口，视频中介绍了一个例子，即垃圾邮件分类。对于垃圾邮件分类，文中的三个字母分别代表：

>**T(task)**: 对垃圾邮件分类这个任务。<br>
**P(Performance)**: 垃圾邮件分类的准确程度。<br>
**E(Experience)**: 用户对于邮件进行是否为垃圾邮件的分类（即帮助机器体验、学习）。<br>

> ### 1.2 监督学习与无监督学习
> 监督学习和无监督学习是机器学习的两种常见的类型，以下是他们的定义：
>> **监督学习** **（Supervised Learning）** 即为教计算机如何去完成预测任务（有反馈），预先给一定数据量的输入和对应的结果，建模拟合，最后让计算机预测未知数据的结果。<br>

>>监督学习一般分为以下两种问题:<br>
>>
>> **回归问题（Classification）** <br>
>> 
>> 回归问题即为预测一系列的连续值。

>> 在房屋价格预测的例子中，给出了一系列的房屋面基数据，根据这些数据来预测任意面积的房屋价格。给出照片-年龄数据集，预测给定照片的年龄。<br>
>> 
>> ![](images/Regression.jpg)
>> 
>> **分类问题（Classification）** <br>
>> 
>> 分类问题即为预测一系列的离散值。即根据数据预测被预测对象属于哪个分类。
>> 
>> 视频中举了癌症肿瘤这个例子，针对诊断结果，分别分类为良性或恶性。还例如垃圾邮件分类问题，也同样属于监督学习中的分类问题。
>> ![](images/Classification.jpg)
> 
>>视频中提到支持向量机这个算法，旨在解决当特征量很大的时候(特征即如癌症例子中的肿块大小，颜色，气味等各种特征)，计算机内存一定会不够用的情况。支持向量机能让计算机处理无限多个特征。<br>
<br>

>>**无监督学习（Unsupervised Learning）**<br>
>>
>>相对于监督学习，训练集不会有人为标注的结果（无反馈），我们不会给出结果或无法得知训练集的结果是什么样，而是单纯由计算机通过无监督学习算法自行分析，从而“得出结果”。

> ## 2. 单变量线性回归（Linear Regression With one Variable）
> ### 2.1 模型描述——房价预测训练集
> ![](images/LinearRegressionModal.jpg)
> 房价预测训练集中，同时给出了输入x和输出结果y，即给出了人为标注的”正确结果“，且预测的量是连续的，属于监督学习中的回归问题。
![](images/LinearRegressionHx.jpg)

>其中 h 代表结果函数，也称为假设(hypothesis) 。这个函数 h 根据输入(房屋的面积)，给出预测结果输出(房屋的价格)，即是一个 X → Y的映射。
>
>![](images/Hx.jpg)<br>
> 上式中，θ为参数，θ的变化才决定了输出结果，不同以往，这里的 x被我们视作已知(不论是数据集还是预测前的输入)，所以怎样解得 θ以更好地拟合数据，成了求解该问题的最终问题。
> 
> 
> 单变量，即只有一个特征(如例子中房屋的面积这个特征)。
> ### 2.2 损失函数（Cost Function）
> ![](images/Jietu20180921-095941.jpg)
> 刚才我们说了h(x)函数的作用是通过给定一个输入x返回一个预测的结果h，而h(x)中两个参数θ1、θ2的取值决定了拟合曲线的好坏程度。因此我们现在所要做的就是要找到拟合最好的θ1、θ2的取值。因此我们引入了损失函数J(θ)。
> 
> ![](images/Jietu20180921-100542.jpg)
> 
> 当我们把数据集表示在坐标系之后，上图中的红色的‘x’即是离散的数据，使用线性回归的方法拟合数据只需要绘制一条离所有的离散点距离最近的直线，如上图所示。因此当唯一确定θ的值即可唯一确定这一条曲线。我们可以用一个式子来表示这一直线的性质，也就是上图右上角的式子。我们都知道直线上对应着X的取值即为预测值。因此我们只需要限制预测值与真实值的平方差之和最小即可最小化θ的值。式子中m为离散数据的数量。如下为损失函数J(θ)。 
> ![](images/Jietu20180921-101641.jpg)
> 
> ### 2.3 损失函数直观理解
> ![](images/Jietu20180921-102209.jpg)
> 为了更加容易理解损失函数到底做了什么，我们先让第一个参数θ0 = 0。当前h(x) = θ1x，拟合出来的直线是一条过原点的直线。现在我们来最小化J(θ)。
>
> ![](images/Jietu20180921-102605.jpg) 
> 
> 假设当前给定一个数据集如上图左边所示，当θ1 = 1的时候，h(x)刚好穿过所有的数据点，也就是说完全拟合。我们通过J(θ)公式算出当前情况下J(θ)的值并在右边的坐标系中标出。
> 
> ![](images/Jietu20180921-103023.jpg)
> 
> ![](images/Jietu20180921-103117.jpg)
> 
> 同理标注出θ取不同的值时损失函数的取值，用一条平滑的曲线连接各点即可得到上图中右侧的图形。我们可以看到J(θ)的函数图像是一个类似于二次函数的图像，开口向上，因此当θ1取值为1的时候函数达到最低点也就是J(θ)取最小值。而当θ1 = 1的时候拟合程度最高。因此说明了损失函数值越小拟合程度越高。
>
> 接下来，我们来讨论θ1和θ2都不为零的情况。
> ![](images/Jietu20180921-103931.jpg)
> ![](images/Jietu20180921-103947.jpg)
> 由上图可以看到，J(θ1，θ2)的图像不再是二次函数，而是一个向下凹的曲面。我们用等高线的表示方法来表示可以更直观的表示。如下图所示：

>![](images/Jietu20180921-104532.jpg)
>![](images/Jietu20180921-104550.jpg)
>![](images/Jietu20180921-104612.jpg)
>![](images/Jietu20180921-104627.jpg)
>
>由上列举了不同θ的取值，当θ位于圆中心的时候拟合的最好也就是J(θ1，θ2)最小。
>
> ### 2.4 梯度下降算法（Gradient Descent）
> 上面介绍了损失函数以及它的直观表示。我们拟合曲线的时候需要使损失函数最小，因此我们引入梯度下降算法。
> ![](images/Jietu20180921-105725.jpg)
> ![](images/Jietu20180921-105740.jpg)
> ![](images/Jietu20180921-105850.jpg)
> 我们首先定义一个学习率α，它决定了梯度下降时沿着梯度每一次迭代的步长。
> 
> 我们让α和损失函数的梯度相乘，并用θj减去他们的乘积即可更新θj的值。我们只需要循环执行这个式子，当下降到局部最小的时候，损失函数的梯度值为零，因此θ的值将不会再改变。
> 
> ![](images/Jietu20180921-110711.jpg)
> 
> **有几点得引起我们的注意：**<br>

> 
> * 学习率α的取值如果太小的话会导致梯度下降变得十分缓慢，而如果学习率过大的话，可能会导致越过最低点并且代价函数的值可能会越来越大。

> ![](images/Jietu20180921-110835.jpg)> 
> 
> * 梯度下降算法可能没法找到全局最低点。当我们从不同的两个点执行梯度下降算法的时候，可能会得到完全不同的两个最小值。因此我们得知道，梯度下降算法往往只能得到一个局部最小值。
> ![](images/Jietu20180921-111332.jpg)
> ![](images/Jietu20180921-111713.jpg)
> 
> ### 2.5 线性回归中的梯度下降（Gradient Descent For Linear Regression）
> 
> ![](images/Jietu20180921-112215.jpg)
> 
> 上图中给出了之前所说的线性回归模型和梯度下降模型。我们将两边式子合在一起，即可得到线性回归中的梯度下降模型。
> ![](images/Jietu20180921-112513.jpg)
> ![](images/Jietu20180921-112529.jpg)
> 
> 以下是迭代过程
> ![](images/Jietu20180921-114858.jpg)
> ![](
> images/Jietu20180921-114914.jpg)
> ![](images/Jietu20180921-114929.jpg)
> ![](images/Jietu20180921-114944.jpg)
> ![](images/Jietu20180921-115002.jpg)
> ![](images/Jietu20180921-115018.jpg)
> ![](images/Jietu20180921-115034.jpg)
> ![](images/Jietu20180921-115047.jpg)
> ![](images/Jietu20180921-115101.jpg)




	
